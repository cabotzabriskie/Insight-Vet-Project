#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Jan 20 16:15:13 2019

@author: cabot
"""


import os, random
import pandas as pd
from selenium import webdriver
from selenium.webdriver.support.select import Select
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import re
import glob
import json
import time

def setOptions():
    options = webdriver.ChromeOptions();
    options.add_argument('--disable-infobars');
    options.add_argument('--disable-dev-shm-usage');
    options.add_argument('--disable-extensions');
    options.add_argument('--headless');
    options.add_argument('--disable-gpu');
    options.add_argument('--no-sandbox');
    options.add_argument('--no-proxy-server')
    options.add_experimental_option("excludeSwitches", ["ignore-certificate-errors"]);
    return options


def getNextPage(driver, wait):
    nextURL = driver.find_element_by_xpath("//a[@class='u-decoration-none next pagination-links_anchor']").get_attribute('href')
    soup = finishLoadGrabSource(nextURL, driver, wait)
    return soup

def finishLoadGrabSource(url, driver, wait):
   
    loadingCondition = EC.invisibility_of_element_located((By.CLASS_NAME,'throbber'))
    try:
        driver.get(url)
        pageLoaded = wait.until(loadingCondition);
        soup = BeautifulSoup(driver.page_source, 'lxml')
        return soup
    except:
        print('Page failed to load, or browser time out')
        pass;

def getBusinessDetails(s):
    return [s.at['biz_url']]

def startDriver():
    options = setOptions()
    driver = webdriver.Chrome(chrome_options=options);  

    wait = WebDriverWait(driver, 30);
    return driver, wait

def quitDriver(driver):
    driver.close();
    driver.quit();

def startThread(city):

    filepath = glob.glob('scraped-data/'+city+'_businesses_v*.csv');

    if(len(filepath)==1):
        path = filepath[0]
        df = pd.read_csv(path)
    elif (len(filepath)>1):
        print('WARNING: more than 1 city file for city')
        return False

    driver, wait = startDriver()
    listOverall=[];

    totalPages = df.index.size
    for i in df.index:
        business=i+1
        print('Scraping Reviews for: ',city,' Business: ',business)


        url='https://www.yelp.com'+df.at[i,'biz_url']
        bizList = getBusinessDetails(df.loc[i])
        
                

        if( (i>0) & (i % 20 == 0)):
            quitDriver(driver)
            driver, wait = startDriver()    
                
        
        
  
        soup = finishLoadGrabSource(url, driver, wait)
        
       
 
        
        for div in soup.find_all("div", class_ = "content-container js-biz-details"):
            try:
                biz_name = div.find("meta", itemprop = "name")
                biz_name = biz_name['content']
            except:
                biz_name="NA"
            try:
                biz_phone = div.find("span", itemprop = "telephone").get_text()
            except:
                biz_phone="NA"
            try:
                biz_rating = div.find("meta", itemprop = "ratingValue")
                biz_rating = biz_rating['content']
            except:
                biz_rating="NA"
        
        currentBizList = []
        bizList = [bizList[0], biz_rating,biz_name,biz_phone]
       
        for div in soup.find_all("div", class_ = "from-biz-owner-content"):
            try:
                biz_desc = div.find("p")
            except:
                biz_desc = "NA"
        
          
        table= soup.find('table', class_ = 'table table-simple hours-table')        
        
        try:
        
            opn = []
            for row in table.find_all("tr"):
    
                cell = row.find("td")
                hr = cell.get_text()

                opn.append(hr)

            mon = opn[0]
            tues = opn[1]
            wed = opn[2]
            thurs = opn[3]
            fri = opn[4]
            sat = opn[5]
            sun = opn[6]
        
        except:
            mon = "NA"
            tues = "NA"
            wed = "NA"
            thurs = "NA"
            fri = "NA"
            sat = "NA"
            sun = "NA"
        bizHrs = [mon,tues,wed,thurs,fri,sat,sun, biz_desc]
        bizList = bizList + bizHrs
        
        while(True):
            
            
            for rev in soup.find_all("div", itemprop = "review"):
             
                try:
                    review_name = rev.find("meta", itemprop = "author")
                    review_name = review_name['content']
                except:
                    review_name="NA"
                try:
                    review_rating = rev.find("meta",  itemprop = "ratingValue")
                    review_rating = review_rating['content']
                except:
                    review_rating = "NA"
                try:
                    review_date = rev.find("meta",  itemprop = "datePublished")
                    review_date = review_date['content']
                except:
                    review_date = "NA"
                try:
                    review_text = rev.find('p', attrs={'itemprop': 'description'}).get_text()
                except:
                    
                    review_text="";
                    continue
                reviewList = [review_name,review_rating,review_date,review_text]
                currentBizList.append(reviewList+bizList)


            try:
                soup = getNextPage(driver, wait)
                time.sleep(3)
            except:

                listOverall=listOverall+currentBizList
                break   

    driver.quit()
    return listOverall


dir_path = 'scraped-data/'
extension = '.csv'
columns = ['review-name','review-rating','review-date','review-text','biz_url',
           'biz_rating','biz_name','biz_phone',
           "mon","tues","wed","thurs","fri","sat","sun","biz_desc"]


cities_csv = pd.read_csv("cities_list.csv", header=None).values.tolist()

for i, city in enumerate(cities_csv):
    city = city[0]
    listOverall = startThread(city)
    output = pd.DataFrame.from_records(listOverall, columns=columns)
    now = time.strftime("%Y%m%d-%H%M%S")
    output_csv = dir_path+city+now+extension
    output.to_csv(output_csv)
